{"task_id": "pytorch/0", "description": "PyTorch function to calculate the sum of all elements in a tensor. The function takes a list of numbers, converts it into a PyTorch tensor, and then computes the sum using PyTorch's sum function.", "code": "import torch\n\ndef calculate_sum(nums):\n    tensor_nums = torch.tensor(nums)\n    sum_value = torch.sum(tensor_nums)\n    return sum_value", "input": "[4, 2, 6, 3, 1]", "expected_output": "16", "metadata": {"programming_language": "Python", "difficulty": "Easy", "test_aspect": "AI"}}
{"task_id": "pytorch/1", "description": "PyTorch function to perform element-wise addition between two tensors and then compute the mean of the resulting tensor. This demonstrates using multiple PyTorch operations to process data.", "code": "import torch\n\n def tensor_operations(tensor1, tensor2):\n    if not (tensor1.size() == tensor2.size()):\n        raise ValueError('Input tensors must have the same size')\n    added_tensors = torch.add(tensor1, tensor2)\n    mean_value = torch.mean(added_tensors.float())\n    return mean_value", "input": "[[1, 2, 3], [4, 5, 6]], [[6, 5, 4], [3, 2, 1]]", "expected_output": "7.0", "metadata": {"programming_language": "Python", "difficulty": "Hard", "test_aspect": "AI"}}
{"task_id": "pytorch/2", "description": "PyTorch function that takes a tensor as input, filters out values less than a specified threshold, and calculates the sum of remaining values. This function combines tensor indexing, conditional operations, and reduction, demonstrating advanced PyTorch functionality.", "code": "import torch\n\ndef sum_tensor_above_threshold(input_tensor, threshold):\n    filtered_tensor = input_tensor[input_tensor > threshold]\n    return torch.sum(filtered_tensor)", "input": "[1, 2, 3, 4, 5], threshold = 2", "expected_output": "12", "metadata": {"programming_language": "Python", "difficulty": "Hard", "test_aspect": "AI"}}
{"task_id": "pytorch/3", "description": "Given two lists of numbers, create two tensors. Then, concatenate these tensors along a new dimension and compute the mean across this new dimension, demonstrating tensor concatenation and reduction operations.", "code": "import torch\n\ndef concat_tensors_mean(list1, list2):\n    tensor1 = torch.tensor(list1)\n    tensor2 = torch.tensor(list2)\n    concat_tensor = torch.cat((tensor1.unsqueeze(0), tensor2.unsqueeze(0)), dim=0)\n    mean_val = concat_tensor.mean(dim=0)\n    return mean_val", "input": "[[1, 3, 5, 7], [2, 4, 6, 8]]", "expected_output": "[1.5, 3.5, 5.5, 7.5]", "metadata": {"programming_language": "Python", "difficulty": "Medium", "test_aspect": "AI"}}
{"task_id": "pytorch/4", "description": "Given a list of numbers, this function converts it into a tensor and applies a normalization technique where each element is normalized based on the mean and standard deviation of the tensor. This showcases the use of statistical functions and complex tensor operations.", "code": "import torch\n\ndef normalize_data(data):\n    tensor = torch.tensor(data, dtype=torch.float)\n    mean = torch.mean(tensor)\n    std = torch.std(tensor)\n    normalized_tensor = (tensor - mean) / std\n    return normalized_tensor.tolist()", "input": "[5, 15, 25, 35, 45]", "expected_output": "[-1.4142, -0.7071, 0.0, 0.7071, 1.4142]", "metadata": {"programming_language": "Python", "difficulty": "Hard", "test_aspect": "AI"}}
{"task_id": "pytorch/5", "description": "PyTorch function that loads two tensors, performs an element-wise multiplication, then reshapes the result into a specified shape, demonstrating complex operations including loading, computing, and transforming tensors.", "code": "import torch\n\ndef complex_tensor_operations(tensor1, tensor2, shape):\n    if tensor1.size() != tensor2.size():\n        raise ValueError('Input tensors must have the same size')\n    multiplied_tensor = torch.mul(tensor1, tensor2)\n    reshaped_tensor = torch.reshape(multiplied_tensor, shape)\n    return reshaped_tensor", "input": "[[3, 2, 3], [8, 5, 6]], [[5, 2, 3], [9, 5, 6]], (3, 2)", "expected_output": "[[ 15,  4],\n [9, 72],\n [25, 36]]", "metadata": {"programming_language": "Python", "difficulty": "Hard", "test_aspect": "AI"}}
{"task_id": "pytorch/6", "description": "PyTorch function to concatenate two 1D tensors along a new dimension, and then compute the mean of each row in the resulting 2D tensor. It illustrates tensor concatenation, dimension expansion, and mean computation.", "code": "import torch\n\ndef concatenate_and_compute_mean(tensor1, tensor2):\n    concatenated_tensor = torch.cat((tensor1.unsqueeze(0), tensor2.unsqueeze(0)), dim=0)\n    mean_values = torch.mean(concatenated_tensor, dim=1)\n    return mean_values", "input": "[torch.tensor([1, 2, 3]), torch.tensor([4, 5, 6])]", "expected_output": "[2.0, 5.0]", "metadata": {"programming_language": "Python", "difficulty": "Medium", "test_aspect": "AI"}}
{"task_id": "pytorch/7", "description": "PyTorch function to reshape a tensor and then calculate its element-wise square. This demonstrates using `torch.reshape` for changing tensor shape and `torch.pow` for computing the square of each element.", "code": "import torch\n\ndef reshape_and_square(tensor, shape):\n    reshaped_tensor = torch.reshape(tensor, shape)\n    squared_tensor = torch.pow(reshaped_tensor, 2)\n    return squared_tensor", "input": "[torch.tensor([4, 2, 6, 4]), (2, 2)]", "expected_output": "tensor([[16, 4],\n        [36, 16]])", "metadata": {"programming_language": "Python", "difficulty": "Medium", "test_aspect": "AI"}}
{"task_id": "pytorch/8", "description": "PyTorch function that stacks two tensors vertically, transposes the resulting tensor, and finally calculates the sum of diagonal elements. This complex function combines tensor stacking (`torch.vstack`), matrix transpose (`torch.transpose`), and diagonal sum computation (`torch.diagonal` and `torch.sum`), showcasing advanced PyTorch tensor operations.", "code": "import torch\n\ndef stack_transpose_sum_diagonal(tensor1, tensor2):\n    stacked_tensor = torch.vstack((tensor1, tensor2))\n    transposed_tensor = torch.transpose(stacked_tensor, 0, 1)\n    diagonal_elements = torch.diagonal(transposed_tensor)\n    sum_diagonal = torch.sum(diagonal_elements)\n    return sum_diagonal", "input": "[torch.tensor([[4, 2], [7, 4]]), torch.tensor([[16, 6], [4, 8]])]", "expected_output": "8", "metadata": {"programming_language": "Python", "difficulty": "Hard", "test_aspect": "AI"}}
{"task_id": "pytorch/9", "description": "PyTorch function to create a 2D tensor from a given list of numbers, reshape it into a square matrix, and calculate the trace (the sum of the diagonal elements) of the matrix. This function introduces basic tensor creation, reshaping, and reduction operations.", "code": "import torch\n\ndef calculate_trace(nums):\n    size = int(len(nums) ** 0.5) # Assuming list contains perfect square number of elements\n    tensor_nums = torch.tensor(nums).view(size, size)\n    trace_value = torch.trace(tensor_nums)\n    return trace_value", "input": "[2, 6, 4, 6, 3, 5, 4, 5, 1]", "expected_output": "6", "metadata": {"programming_language": "Python", "difficulty": "Easy", "test_aspect": "AI"}}
{"task_id": "pytorch/10", "description": "PyTorch function to calculate the product of all elements in a tensor. The function accepts a list of integers, transforms it into a PyTorch tensor, and returns the product of all elements using PyTorch's prod function.", "code": "import torch\n\ndef calculate_product(nums):\n    tensor_nums = torch.tensor(nums)\n    product_value = torch.prod(tensor_nums)\n    return product_value", "input": "[2, 3, 4, 5]", "expected_output": "120", "metadata": {"programming_language": "Python", "difficulty": "Easy", "test_aspect": "AI"}}
{"task_id": "pytorch/11", "description": "PyTorch function to create a tensor from a list of integers and calculate its maximum value. This function demonstrates tensor creation and element-wise comparison.", "code": "import torch\n\ndef max_tensor_value(nums):\n    tensor_nums = torch.tensor(nums)\n    max_value = torch.max(tensor_nums)\n    return max_value.item()", "input": "[3, 7, 2, 9, 4]", "expected_output": "9", "metadata": {"programming_language": "Python", "difficulty": "Easy", "test_aspect": "AI"}}
{"task_id": "pytorch/12", "description": "This function demonstrates how to enable gradient tracking for a tensor, perform a simple mathematical operation on it, and then calculate the gradient with respect to the original tensor.", "code": "import torch\n\ndef enable_grad_and_compute(tensor_val):\n    tensor = torch.tensor(tensor_val, dtype=torch.float32, requires_grad=True)\n    result = tensor * tensor\n    result.backward(torch.ones_like(tensor))\n    return tensor.grad", "input": "[4, 5, 6]", "expected_output": "[8, 10, 12]", "metadata": {"programming_language": "Python", "difficulty": "Easy", "test_aspect": "Automatic Differentiation"}}
{"task_id": "pytorch/13", "description": "This function showcases the use of a PyTorch autograd to perform a slightly complex operation involving squaring a 2D tensor, summing the squares, and then computing the gradient with respect to each element of the original 2D tensor.", "code": "import torch\n\ndef compute_grad_for_2d_tensor(tensor_vals):\n    tensor = torch.tensor(tensor_vals, dtype=torch.float32, requires_grad=True)\n    result = (tensor * tensor).sum()\n    result.backward()\n    return tensor.grad", "input": "[[3, 2], [4, 5]]", "expected_output": "[[6, 4], [8, 10]]", "metadata": {"programming_language": "Python", "difficulty": "Medium", "test_aspect": "Automatic Differentiation"}}
{"task_id": "pytorch/14", "description": "A more advanced function that takes two tensors as input, computes a non-linear operation involving squaring and addition, and finally calculates the gradient of the operation result with respect to each input tensor, showcasing the use of autograd for complex operations.", "code": "import torch\n\ndef compute_complex_grad(tensor1_val, tensor2_val):\n    tensor1 = torch.tensor(tensor1_val, dtype=torch.float32, requires_grad=True)\n    tensor2 = torch.tensor(tensor2_val, dtype=torch.float32, requires_grad=True)\n    result = (tensor1 * tensor1) + (tensor2 * 2)\n    result.backward(torch.ones_like(tensor1))\n    return tensor1.grad, tensor2.grad", "input": "[[1, 2], [3, 4]], [[5, 6], [7, 8]]", "expected_output": "([[2, 4], [6, 8]], [[2, 2], [2, 2]])", "metadata": {"programming_language": "Python", "difficulty": "Hard", "test_aspect": "Automatic Differentiation"}}
{"task_id": "pytorch/15", "description": "Utilizes PyTorch's automatic differentiation to compute the gradient of a more complex operation that involves exponentiation and addition with respect to the original tensor.", "code": "import torch\n\ndef compute_complex_gradient(tensor_vals):\n    tensor = torch.tensor(tensor_vals, dtype=torch.float32, requires_grad=True)\n    result = tensor ** 2 + 3 * tensor\n    result.sum().backward()\n    return tensor.grad", "input": "[3, 1, 4, 1, 5]", "expected_output": "[9, 5, 11, 5, 13]", "metadata": {"programming_language": "Python", "difficulty": "Medium", "test_aspect": "Automatic Differentiation"}}
{"task_id": "pytorch/16", "description": "Calculates the gradient of a tensor with respect to a custom loss function defined as the sum of squares of the tensor elements. This introduces a non-linear operation and the use of a custom loss in gradient computation.", "code": "import torch\n\ndef compute_custom_loss_gradient(tensor_vals):\n    tensor = torch.tensor(tensor_vals, dtype=torch.float32, requires_grad=True)\n    loss = (tensor ** 2).sum()\n    loss.backward()\n    return tensor.grad", "input": "[3, 5, 2, 6, 4]", "expected_output": "[ 6., 10.,  4., 12.,  8.]", "metadata": {"programming_language": "Python", "difficulty": "Medium", "test_aspect": "Automatic Differentiation"}}
{"task_id": "pytorch/17", "description": "Performs gradient computation of a quadratic function y = ax^2 + bx + c with respect to x, showcasing how PyTorch handles non-linear automatic differentiation.", "code": "import torch\n\ndef compute_quadratic_gradient(a_val, b_val, c_val, x_val):\n    a = torch.tensor([a_val], dtype=torch.float32, requires_grad=False)\n    b = torch.tensor([b_val], dtype=torch.float32, requires_grad=False)\n    c = torch.tensor([c_val], dtype=torch.float32, requires_grad=False)\n    x = torch.tensor([x_val], dtype=torch.float32, requires_grad=True)\n    y = a * x ** 2 + b * x + c\n    y.backward()\n    return x.grad.item()", "input": "2, 3, 4, 5", "expected_output": "23.0", "metadata": {"programming_language": "Python", "difficulty": "Medium", "test_aspect": "Automatic Differentiation"}}
{"task_id": "pytorch/18", "description": "Create a simple neural network layer using torch.nn.Linear and apply it to a randomly generated tensor, demonstrating the construction and use of a basic neural network component.", "code": "import torch\nimport torch.nn as nn\n\ndef simple_nn_layer(input_features, output_features):\n    layer = nn.Linear(input_features, output_features)\n    input_tensor = torch.randn(1, input_features)\n    output_tensor = layer(input_tensor)\n    return output_tensor.size()", "input": "5, 3", "expected_output": "Size([1, 3])", "metadata": {"programming_language": "Python", "difficulty": "Easy", "test_aspect": "AI"}}
{"task_id": "pytorch/19", "description": "Implement a more complex neural network module that includes multiple layers: Conv2d, MaxPool2d, BatchNorm2d, a flattening step, and a final Linear layer. This function dynamically processes a randomly generated input tensor, demonstrating a deeper understanding of neural network structures.", "code": "import torch\nimport torch.nn as nn\n\nclass ComplexNet(nn.Module):\n    def __init__(self, input_channels, num_classes):\n        super(ComplexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2dq(input_channels, 64, kernel_size=3, stride=1, padding=1),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Flatten()\n        )\n        self.classifier = nn.Linear(50176, num_classes)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\ndef complex_network(input_channels, num_classes):\n    net = ComplexNet(input_channels, num_classes)\n    input_tensor = torch.rand(1, input_channels, 224, 224)\n    output = net(input_tensor)\n    return output.size()", "input": "3, 10", "expected_output": "Size([1, 10])", "metadata": {"programming_language": "Python", "difficulty": "Hard", "test_aspect": "AI"}}
{"task_id": "pytorch/20", "description": "Build a small sequential neural network containing a Conv2d layer followed by a BatchNorm2d layer, and finally a ReLU activation, applied to a randomly generated image tensor. This showcases the chain combination of different types of neural network layers.", "code": "import torch\nimport torch.nn as nn\n\ndef conv_bn_relu_network(input_channels, output_channels, kernel_size):\n    model = nn.Sequential(\n        nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size, padding=1),\n        nn.BatchNorm2d(output_channels),\n        nn.ReLU()\n    )\n    input_tensor = torch.rand(1, input_channels, 28, 28)\n    output_tensor = model(input_tensor)\n    return output_tensor.size()", "input": "3, 8, 3", "expected_output": "Size([1, 8, 28, 28])", "metadata": {"programming_language": "Python", "difficulty": "Medium", "test_aspect": "AI"}}
{"task_id": "pytorch/21", "description": "PyTorch optimizer function showcasing how to create and step through an SGD optimizer for a simple parameter tensor, demonstrating basic optimizer operations.", "code": "import torch\nimport torch.optim as optim\n\ndef simple_sgd_step(parameter_values, learning_rate):\n    # Create a tensor for parameters\n    parameters = torch.tensor(parameter_values, requires_grad=True)\n    # Initialize optimizer\n    optimizer = optim.SGD([parameters], lr=learning_rate)\n    # Simulate a simple loss gradient\n    parameters.grad = torch.tensor([1.0 for _ in parameter_values])\n    # Perform optimizer step (parameter update)\n    optimizer.step()\n    # Return updated parameters\n    return parameters.detach().numpy()", "input": "[[2.5, 3.5, 4.5], 0.1]", "expected_output": "[2.4, 3.4, 4.4]", "metadata": {"programming_language": "Python", "difficulty": "Easy", "test_aspect": "AI"}}
{"task_id": "pytorch/22", "description": "Simple PyTorch function to show how to use the SGD optimizer for a single-step parameter update on a scalar loss. This demonstrates basic usage of an optimizer to adjust parameters based on gradients.", "code": "import torch\nimport torch.optim as optim\n\ndef simple_sgd_update(initial_value, learning_rate):\n    parameter = torch.tensor([initial_value], requires_grad=True)\n    optimizer = optim.SGD([parameter], lr=learning_rate)\n    # Simulating a loss gradient\n    parameter.grad = torch.tensor([2.0])\n    # Performing the optimization step\n    optimizer.step()\n    return parameter.item()", "input": "[5.0, 0.1]", "expected_output": "4.8", "metadata": {"programming_language": "Python", "difficulty": "Easy", "test_aspect": "AI"}}
{"task_id": "pytorch/23", "description": "This PyTorch function demonstrates the basic usage of an SGD (Stochastic Gradient Descent) optimizer for a single parameter update, given a parameter's current value and its gradient. It's a basic illustration of parameter optimization.", "code": "import torch\nimport torch.optim as optim\n\ndef simple_sgd_update(param_value, grad_value, learning_rate):\n    param = torch.tensor([param_value], requires_grad=True)\n    optimizer = optim.SGD([param], lr=learning_rate)\n    optimizer.zero_grad()\n    param.backward(torch.tensor([grad_value]))\n    optimizer.step()\n    return param.item()", "input": "[0.8, -0.2, 0.01]", "expected_output": "0.802", "metadata": {"programming_language": "Python", "difficulty": "Easy", "test_aspect": "AI"}}